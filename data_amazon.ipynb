{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "seed = 42\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def dump_json(data, path):\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dir(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "\n",
    "def get_timestamp(date_format: str = \"%d%H%M%S\") -> str:\n",
    "    timestamp = datetime.now()\n",
    "    return timestamp.strftime(date_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_core = 5\n",
    "data_dir = f\"./data/home\"\n",
    "mk_dir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, \"rb\")\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in tqdm(parse(path)):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient=\"index\")\n",
    "\n",
    "\n",
    "# download raw data from https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html\n",
    "meta_df = getDF(\"./meta_Home_and_Kitchen.json.gz\") \n",
    "inter_df = getDF(\"./reviews_Home_and_Kitchen_5.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[meta_df.asin.isin(inter_df.asin.unique())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_df.reviewerID.nunique()\n",
    "inter_df.asin.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = meta_df[[\"imUrl\", \"asin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=0.3, status_forcelist=[500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "session = create_session()\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
    "}\n",
    "no_img_list = []\n",
    "\n",
    "\n",
    "def download_and_save_image(row, img_path):\n",
    "    try:\n",
    "        images = row.imUrl\n",
    "        parent_asin = row.asin\n",
    "\n",
    "        response = session.get(images, headers=headers, timeout=10)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "        img.save(f\"{img_path}/{parent_asin}.jpg\", \"JPEG\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {row.asin}: {e}, save\")\n",
    "        no_img_list.append(row.asin)\n",
    "        torch.save(no_img_list, f\"{data_dir}/no_img.pt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row.asin}: {e}, save\")\n",
    "        no_img_list.append(row.asin)\n",
    "        torch.save(no_img_list, f\"{data_dir}/no_img.pt\")\n",
    "\n",
    "\n",
    "# Image save path\n",
    "img_path = f\"{data_dir}/images\"\n",
    "mk_dir(img_path)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    for _ in tqdm(\n",
    "        executor.map(\n",
    "            lambda row: download_and_save_image(row, img_path),\n",
    "            meta_df[[\"imUrl\", \"asin\"]].itertuples(index=False),\n",
    "        ),\n",
    "        total=len(meta_df),\n",
    "    ):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_img_data = meta_df[meta_df[\"asin\"].isin(no_img_list)]\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for _ in tqdm(\n",
    "        executor.map(\n",
    "            lambda row: download_and_save_image(row, img_path),\n",
    "            no_img_data[[\"imUrl\", \"asin\"]].itertuples(index=False),\n",
    "        ),\n",
    "        total=len(no_img_data),\n",
    "    ):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[~meta_df[\"asin\"].isin(no_img_list)].reset_index(drop=True)\n",
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_data = inter_df[inter_df[\"asin\"].isin(meta_df[\"asin\"])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-core_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_df\n",
    "inter_data = ava_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_data.columns)\n",
    "print(meta_data.shape)\n",
    "print(inter_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_checker(df, group, target, threshold):\n",
    "    counter = df.groupby(group)[target].nunique()\n",
    "    valid = counter[counter >= threshold].index\n",
    "    return df[df[group].isin(valid)]\n",
    "\n",
    "\n",
    "def data_cutter(origin_data, user_core, item_core):\n",
    "    print(\"### before ###\")\n",
    "    print(\"shape of n_interaction_data : \", origin_data.shape)\n",
    "\n",
    "    while True:\n",
    "        new_data = core_checker(origin_data, \"reviewerID\", \"asin\", user_core)\n",
    "        new_data = core_checker(new_data, \"asin\", \"reviewerID\", item_core)\n",
    "\n",
    "        if new_data.equals(origin_data):\n",
    "            print(\"finish\")\n",
    "            break\n",
    "\n",
    "        origin_data = new_data\n",
    "\n",
    "    print(\"### after all item sampled ###\")\n",
    "    print(f\"### user_core : {user_core}, item_core : {item_core} ###\")\n",
    "    print(\"shape of n_interaction_data : \", new_data.shape)\n",
    "    print(\"num of user : \", new_data.reviewerID.nunique())\n",
    "    print(\"num of item : \", new_data.asin.nunique())\n",
    "    print(\n",
    "        \"data density : \",\n",
    "        new_data.shape[0]\n",
    "        / (new_data.reviewerID.nunique() * new_data.asin.nunique())\n",
    "        * 100,\n",
    "        \"%\",\n",
    "    )\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### before ###\n",
      "shape of n_interaction_data :  (550457, 9)\n",
      "finish\n",
      "### after all item sampled ###\n",
      "### user_core : 5, item_core : 5 ###\n",
      "shape of n_interaction_data :  (548769, 9)\n",
      "num of user :  66167\n",
      "num of item :  28125\n",
      "data density :  0.02948869778993557 %\n"
     ]
    }
   ],
   "source": [
    "core_inter_data = data_cutter(inter_data, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_inter_data.to_csv(f\"./{data_dir}/{n_core}_core_inter_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_meta_data = meta_data[meta_data[\"asin\"].isin(core_inter_data[\"asin\"])].reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_meta_data.to_csv(f\"./{data_dir}/{n_core}_core_meta_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of n_item_data : \", core_meta_data.shape)\n",
    "print(\"shape of new_interaction_data : \", core_inter_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"shape of interaction data\": core_inter_data.shape,\n",
    "    \"user_core\": 5,\n",
    "    \"item_core\": 5,\n",
    "    \"shape of meta_data\": core_meta_data.shape,\n",
    "    \"num of user\": core_inter_data.reviewerID.nunique(),\n",
    "    \"num of item\": core_inter_data.asin.nunique(),\n",
    "    \"data density\": f\"{core_inter_data.shape[0]/(core_inter_data.reviewerID.nunique()*core_inter_data.asin.nunique())*100}%\",\n",
    "}\n",
    "\n",
    "dump_json(metadata, f\"{data_dir}/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interaction_data = pd.read_csv(f\"{data_dir}/{n_core}_core_inter_data.csv\")\n",
    "n_item_data = pd.read_csv(f\"{data_dir}/{n_core}_core_meta_data.csv\")\n",
    "\n",
    "print(\"shape of n_item_data : \", n_item_data.shape)\n",
    "print(\"shape of new_interaction_data : \", new_interaction_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "fclip = FashionCLIP(\"fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = (\n",
    "    n_item_data[\"asin\"]\n",
    "    .progress_apply(lambda x: f\"{data_dir}/images/{x}.jpg\")\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fclip = fclip.encode_images(images, batch_size=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {article_id : emb}\n",
    "id_img_emb_map = {\n",
    "    k: torch.tensor(v) for k, v in zip(n_item_data[\"asin\"].tolist(), image_fclip)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_data[\"categories\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_data[\"categories_f\"] = n_item_data[\"categories\"].progress_apply(\n",
    "    lambda x: eval(x)\n",
    ")\n",
    "n_item_data[\"categories_f\"] = n_item_data[\"categories_f\"].progress_apply(\n",
    "    lambda x: x[0] if isinstance(x, list) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = [y for x in n_item_data[\"categories_f\"] for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(cat_list).most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(n_item_data.iterrows(), total=len(n_item_data)):\n",
    "    if \"Home & Kitchen\" in n_item_data.at[idx, \"categories_f\"]:\n",
    "        n_item_data.at[idx, \"categories_f\"].remove(\"Home & Kitchen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in tqdm(n_item_data.iterrows(), total=len(n_item_data)):\n",
    "#     if \"Clothing, Shoes & Jewelry\" in n_item_data.at[idx, \"categories_f\"]:\n",
    "#         n_item_data.at[idx, \"categories_f\"].remove(\"Clothing, Shoes & Jewelry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_data[\"categories_f\"] = n_item_data[\"categories_f\"].apply(\n",
    "    lambda x: x if len(x) else [\"Unknown\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_data[\"description\"] = n_item_data[\"description\"].fillna(\" \")\n",
    "n_item_data[\"title\"] = n_item_data[\"title\"].fillna(\" \")\n",
    "n_item_data[\"brand\"] = n_item_data[\"brand\"].fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i, row in n_item_data.iterrows():\n",
    "    sen = row[\"title\"] + \" \" + row[\"brand\"] + \" \"\n",
    "    cates = eval(row[\"categories\"])\n",
    "    if isinstance(cates, list):\n",
    "        for c in cates[0]:\n",
    "            sen = sen + c + \" \"\n",
    "    sen += row[\"description\"]\n",
    "    sen = sen.replace(\"\\n\", \" \")\n",
    "\n",
    "    sentences.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_data[\"sentences\"] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fclip = fclip.encode_text(n_item_data[\"sentences\"], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_text_emb_map_t = {\n",
    "    k: torch.tensor(v) for k, v in zip(n_item_data[\"asin\"].tolist(), t_fclip)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2idx = {\n",
    "    v: k for k, v in enumerate(new_interaction_data[\"reviewerID\"].unique())\n",
    "}  # {reviewerIDrID:idx}\n",
    "item2idx = {v: k for k, v in enumerate(n_item_data[\"asin\"].unique())}  # {item_id:idx}\n",
    "\n",
    "\n",
    "print(\"# of user\", len(user2idx))\n",
    "print(\"# of item\", len(item2idx))\n",
    "\n",
    "torch.save(item2idx, f\"{data_dir}/item2idx.pt\")\n",
    "torch.save(user2idx, f\"{data_dir}/user2idx.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_img_emb_map = {\n",
    "    item2idx[row[\"asin\"]]: id_img_emb_map[row[\"asin\"]]\n",
    "    for _, row in tqdm(n_item_data.iterrows(), total=len(n_item_data))\n",
    "}\n",
    "idx_text_emb_map = {\n",
    "    item2idx[row[\"asin\"]]: id_text_emb_map_t[row[\"asin\"]]\n",
    "    for _, row in tqdm(n_item_data.iterrows(), total=len(n_item_data))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(idx_img_emb_map, f\"{data_dir}/idx_img_emb_map.pt\")\n",
    "torch.save(idx_text_emb_map, f\"{data_dir}/idx_text_emb_map.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_iteraction_data = new_interaction_data.sort_values(\n",
    "    [\"reviewerID\", \"unixReviewTime\"]\n",
    ").reset_index(drop=True)\n",
    "sorted_iteraction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_iteraction_data[\"reviewerID\"] = sorted_iteraction_data[\"reviewerID\"].map(\n",
    "    user2idx\n",
    ")\n",
    "sorted_iteraction_data[\"asin\"] = sorted_iteraction_data[\"asin\"].map(item2idx)\n",
    "sorted_iteraction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_iteraction_data = sorted_iteraction_data[[\"reviewerID\", \"asin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data = sorted_iteraction_data.drop_duplicates(\n",
    "    [\"asin\", \"reviewerID\"], keep=\"last\"\n",
    ")\n",
    "min(unique_data.groupby([\"\"]).count()[\"asin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"shape of interaction data\": unique_data.shape,\n",
    "    \"user_core\": 5,\n",
    "    \"item_core\": 5,\n",
    "    \"num of user\": unique_data.reviewerID.nunique(),\n",
    "    \"num of item\": unique_data.asin.nunique(),\n",
    "    \"data density\": f\"{unique_data.shape[0]/(unique_data.reviewerID.nunique()*unique_data.asin.nunique())*100}%\",\n",
    "}\n",
    "\n",
    "dump_json(metadata, f\"{data_dir}/uniqued_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dict(unique_data.groupby(\"reviewerID\")[\"asin\"].progress_apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [v for v in test_data.values()]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_data, f\"{data_dir}/uniqued_test_data.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
